{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxTdJrU8+u9+ktjoOC56rG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Tareas/Tarea%2012%20RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tarea 12</h1>\n",
        "<h2>Comparativa entre diferentes enfoques para responder preguntas usando LLMs y RAG</h2>\n",
        "\n",
        "----\n",
        "\n",
        "<h3>Instrucciones Generales</h3>\n",
        "\n",
        "**Esta actividad será en parejas**\n",
        "\n",
        "1. **Eligir un tema de tu interés**.\n",
        "\n",
        "2. **Buscar al menos 3 PDFs con información confiable** relacionados con ese tema. Pueden ser:\n",
        "   - Artículos científicos\n",
        "   - Reportes oficiales\n",
        "   - Documentos institucionales\n",
        "   - Blogs bien estructurados (convertidos a PDF)\n",
        "\n",
        "   Puedes usar otros formatos de archivo (texto, tablas, etc.), en dicho caso, hay que averiguar como hacer el chunking con ellos.\n",
        "\n",
        "3. **Formular 3 preguntas específicas** sobre el tema que hayas elegido. Las preguntas no deben ser triviales.\n",
        "\n",
        "4. **Realizar los siguientes experimentos con las preguntas y documentos:**\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Experimento 1</h3>\n",
        "Consultar directamente a un LLM sin usar los archivos\n",
        "\n",
        "- Usar un LLM como Qwen, Claude, ChatGPT, LeChat, DeepSeek, Gemini, etc.\n",
        "- Realizar cada pregunta directamente al modelo sin subir ningún documento.\n",
        "- Evaluar si la respuesta es útil, precisa, relevante o si el modelo reconoce que no tiene suficiente información.\n",
        "\n",
        "**Pregunta**: ¿Cómo explica este modelo lo que no sabe? ¿Da información errónea o evita responder? *Responder en la notebook*\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Experimento 2</h3>\n",
        "Consultar a un LLM subiendo los PDFs directamente\n",
        "\n",
        "- Usar un LLM que permita cargar documentos (ChatGPT , Claude, Google Gemini, DeepSeek, etc).\n",
        "- Subir los PDFs que recolectaste.\n",
        "- Volver a hacer las mismas 3 preguntas.\n",
        "- Analizar si hubo mejora en la calidad de las respuestas.\n",
        "\n",
        "**Pregunta**: ¿Cómo cree que el modelo procesa los PDFs internamente? ¿Es igual que RAG? *Responder en la notebook*\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Experimento 3</h3>\n",
        "Consultar a un LLM local o importado desde Hugging Face\n",
        "\n",
        "- Importar un modelo desde Hugging Face (con `transformers`) o usarlo directamente en la página de HuggingFace (rescuerda usar uno `instruct`).\n",
        "- No usar embeddings ni base de datos vectorial, hacer prompting directamente.\n",
        "- Realizar las mismas tres preguntas, pero **sin usar RAG**, solo con el modelo.\n",
        "\n",
        "**Pregunta**: ¿Qué limitaciones tienen estos modelos sin conexión a documentos externos? *Responder en la notebook*\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Experimento 4</h3>\n",
        "Ahora implementarán un sistema básico de **RAG** siguiendo esta plantilla:\n",
        "\n",
        "<h4>Pasos</h4>\n",
        "\n",
        "1. **Carga y preprocesamiento de PDFs**: Extraer texto y dividirlo en chunks (fragmentos). Probar con al menos dos tamaños de chunk.\n",
        "\n",
        "2. **Embeddings**: Usar dos modelos de incrustación como `sentence-transformers/all-MiniLM-L6-v2`, `BAAI/bge-base-en-v1.5`, u otros (en la notebook hay varias opciones).\n",
        "\n",
        "3. **Base de datos vectorial**: Usar FAISS o ChromaDB para almacenar y recuperar los embeddings.\n",
        "\n",
        "4. **Generador (LLM)**\n",
        "   - Usar un modelo pequeño de Hugging Face (`tiiuae/falcon-7b-instruct`, `meta-llama/Llama-3.2-3B-Instruct`, etc.)\n",
        "   - Probar con al menos dos modelos distintos y comparar.\n",
        "\n",
        "5. **Prompt personalizado**: Diseñar un prompt específico para el RAG, donde se le entregue el contexto extraído y se formule la pregunta. Ejemplo:\n",
        "     ```text\n",
        "     Contexto: [contexto_extraído]\n",
        "     Pregunta: [pregunta]\n",
        "     Respuesta:\n",
        "     ```\n",
        "     También puedes usar el prompt usando en la notebook o algún otro de tu elección.\n",
        "\n",
        "6. **Resumiendo**: Probar variando parámetros en las distintas etapas.\n",
        "   - Chunk size\n",
        "   - Modelo de embeddings (al menos dos)\n",
        "   - Base de datos vectorial (usar FAISS por ejemplo)\n",
        "   - Modelos LLM (probar al menos dos)\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Evaluación de las respuestas</h3>\n",
        "\n",
        "Para cada prueba, deben **evaluar las respuestas** usando los siguientes criterios:\n",
        "\n",
        "| Criterio | Descripción |\n",
        "|--------|-------------|\n",
        "| **Relevancia** | ¿La respuesta aborda directamente la pregunta? |\n",
        "| **Precisión** | ¿Los datos o afirmaciones coinciden con los documentos? |\n",
        "| **Claridad** | ¿La respuesta es fácil de entender? |\n",
        "| **Soporte documental** | ¿Se menciona o se basa en los documentos consultados? |\n",
        "| **Velocidad de respuesta** | ¿Cuánto tiempo tomó obtener la respuesta? |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Reflexión Final</h3>\n",
        "\n",
        "Después de realizar todas las pruebas, respondan las siguientes preguntas:\n",
        "\n",
        "1. ¿Por qué crees que RAG mejora significativamente las respuestas frente a consultar solo el LLM?\n",
        "2. ¿Cuál fue la configuración (chunk size + embeddings + LLM) que dio mejores resultados? ¿Por qué crees que funcionó mejor?\n",
        "3. ¿Qué diferencias notaste entre subir los PDFs directamente a un LLM online y usar RAG con tus propios documentos?\n",
        "4. ¿Qué ventajas encuentras en construir tu propio sistema de RAG en lugar de depender únicamente de funciones integradas en plataformas comerciales?\n",
        "5. ¿Cómo podrías medir objetivamente la calidad de las respuestas?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eOqU8Y9ff8ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeeGD9LIM4G1"
      },
      "outputs": [],
      "source": []
    }
  ]
}