{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkXdVaPkV7rMpDPnGVu57z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Tareas/TC10-DocumentEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo en clase : Word Embeddings y Clasificación de Texto\n",
        "\n",
        "**Objetivo**\n",
        "\n",
        "Entrenar modelos de Word2Vec/FastText y Doc2Vec sobre el corpus de opiniones turísticas, generar representaciones vectoriales de documentos, y evaluar su rendimiento en una tarea de clasificación usando el F1-score.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cm3DEkxICSA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 1: Word2Vec o FastText + Promedios + Clasificación\n",
        "\n",
        "0. Separa el corpus de acuerdo a los parametros definidos anteriormente.\n",
        "1. Preprocesamiento del texto: Tokeniza y elimina stopwords del corpus.\n",
        "2. Entrena un modelo de Word2Vec o FastText en el corpus de entrenamiento.\n",
        "3. Para obtener el embedding de cada documento promedio los embeddings de las palabras que forman al documento.\n",
        "4. Realiza la tarea de clasificación que te tocó con este esquema. Reporta la matriz de clasificación y el F1-score."
      ],
      "metadata": {
        "id": "xWEgBJweCmN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El corpus debe tokenizarse. Cada documento es una lista de tokens y el corpus es una lista de listas\n",
        "tokenized_corpus = ..."
      ],
      "metadata": {
        "id": "uGNfHqBapGxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**"
      ],
      "metadata": {
        "id": "Tjhbfok_qIzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RuiHQYqHxit"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model_w2v = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=,\n",
        "    window=,\n",
        "    min_count=,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3**"
      ],
      "metadata": {
        "id": "kBXMeasUqMHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def document_embedding(tokens, model):\n",
        "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)\n",
        "\n",
        "X_embeddings = np.array([document_embedding(doc, model_w2v) for doc in tokenized_corpus])"
      ],
      "metadata": {
        "id": "_CYIw5uZeUCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 2: Doc2Vec + Clasificación\n",
        "\n",
        "0. Separa el corpus de acuerdo a los parametros definidos anteriormente.\n",
        "1. Preprocesamiento del texto: Tokeniza y elimina stopwords del corpus.\n",
        "2. Entrena un modelo de Doc2Vec en el corpus de entrenamiento.\n",
        "3. Realiza la tarea de clasificación que te tocó con este esquema. Reporta la matriz de clasificación y el F1-score."
      ],
      "metadata": {
        "id": "sNGwdQz9eZSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**"
      ],
      "metadata": {
        "id": "maXE279iqN96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tagged_data = [TaggedDocument(words=doc, tags=[str(i)])\n",
        "               for i, doc in enumerate(tokenized_corpus)]\n",
        "\n",
        "# Parametros\n",
        "vector_size =   # Dimensionality of the feature vectors\n",
        "window =         # Maximum distance between current and predicted word within a sentence\n",
        "min_count =      # Ignores all words with total frequency lower than this\n",
        "workers =        # Use `workers` number of worker threads to train the model\n",
        "epochs =        # Number of iterations (epochs) over the corpus\n",
        "\n",
        "d2v_model = Doc2Vec(vector_size=vector_size,\n",
        "                 window=window,\n",
        "                 min_count=min_count,\n",
        "                 workers=workers)\n",
        "\n",
        "d2v_model.build_vocab(tagged_data)\n",
        "\n",
        "# Entrenamiento\n",
        "d2v_model.train(tagged_data, total_examples=d2v_model.corpus_count, epochs=epochs)"
      ],
      "metadata": {
        "id": "DjdJjUHNervz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3** Los vectores de los documentos"
      ],
      "metadata": {
        "id": "h3YBnZLLqPf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2v_vectors = d2v_model.dv.vectors\n",
        "d2v_vectors.shape"
      ],
      "metadata": {
        "id": "NQjwM2fLnYPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlkQ2IdbesMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}