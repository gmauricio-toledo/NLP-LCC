{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Notebooks/05-Ngramas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBW3_NymG21F"
      },
      "source": [
        "#Modelos de lenguaje: $N$-gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVrmwiUaFEvS"
      },
      "source": [
        "Usaremos el corpus Reuters para entrenar un primer modelo de lenguaje. Este modelo lo usaremos para predicir la siguiente palabra en una secuencia de palabras dada.\n",
        "\n",
        "El Corpus Reuters contiene 10,788 documentos de noticias con un total de 1.3 millones de palabras. Los documentos se han clasificado en 90 temas y agrupado en dos conjuntos, *train* y *test*.\n",
        "\n",
        "* Corpus en NLTK: https://www.nltk.org/howto/corpus.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBG8dzYt-04b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.corpus import reuters\n",
        "from collections import defaultdict\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wjtYJk7-250"
      },
      "outputs": [],
      "source": [
        "nltk.download('reuters')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NicPqSW5Sgcq"
      },
      "source": [
        "## 3-gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ervpo4r2xfQ"
      },
      "source": [
        "### Exploración del corpus\n",
        "\n",
        "Es importante explorar y conocer los corpus. Al entrenar nuestros modelos podemos necesitar corpora con caracteristicas particulares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b8q9JM1FVA8"
      },
      "source": [
        "**Exploremos un poco el corpus**. Veamos el inicio del corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VIpOhKRFWzm"
      },
      "outputs": [],
      "source": [
        "' '.join(reuters.words()[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔶 Mayúsculas"
      ],
      "metadata": {
        "id": "lnZfXA3B92Ja"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCTK4DTzDdr"
      },
      "source": [
        "Podemos acceder a los *ids* de los archivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULAolAOiyEBx"
      },
      "outputs": [],
      "source": [
        "reuters.fileids()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmm8cPI4zIG1"
      },
      "source": [
        "Podemos obtener el tema de cada archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsJFxxsWxC4L"
      },
      "outputs": [],
      "source": [
        "reuters.categories('test/14826')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4wqHklNyM2T"
      },
      "outputs": [],
      "source": [
        "fileids = reuters.fileids()\n",
        "\n",
        "for fileid in fileids[:5]:\n",
        "    print(reuters.categories(fileid))\n",
        "    print(reuters.raw(fileid)[:200],end=\"...\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAft7Tzw04Yt"
      },
      "source": [
        "Podemos ver los documentos que tienen que ver con un tema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxNXosRS0z0z"
      },
      "outputs": [],
      "source": [
        "reuters.fileids('corn')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Z42F4S3ALi"
      },
      "source": [
        "### Crear el modelo de lenguaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUh7t8P-MsV"
      },
      "source": [
        "Tokenizamos el texto quitando signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm8eHcPwh-qm"
      },
      "outputs": [],
      "source": [
        "punctuations = list(punctuation)\n",
        "\n",
        "words = [t for t in nltk.word_tokenize(' '.join(reuters.words())) if t not in punctuations]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeefiltmFnMj"
      },
      "source": [
        "Extraemos los trigramas\n",
        "\n",
        "https://www.nltk.org/api/nltk.util.html#nltk.util.trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9TRx0M0_HOX"
      },
      "outputs": [],
      "source": [
        "tri_grams = list(trigrams(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guJQeFgzFqRG"
      },
      "source": [
        "Veamos los primero trigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ3zmb_1_JUM"
      },
      "outputs": [],
      "source": [
        "print(tri_grams[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9b_b9LuFsyE"
      },
      "source": [
        "Realicemos los conteos. Los almacenamos en un diccionario donde cada llave es un par (las primeras dos palabras del trigrama) y el valor es un diccionario con el conteo de cada tercer palabra.\n",
        "\n",
        "Usamos un [`defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict) para almacenar estos conteos. En caso de que una tripleta no exista, tendremos el valor 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzQGoekZAAZ4"
      },
      "outputs": [],
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0)) # Valor por defecto si el trigrama no existe\n",
        "print(model)\n",
        "\n",
        "for w1, w2, w3 in tri_grams:\n",
        "    model[(w1, w2)][w3] += 1\n",
        "    break\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuwgLg5QAwyp"
      },
      "source": [
        "Veamos cómo se ve este diccionario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emRIZmvyAbsa"
      },
      "outputs": [],
      "source": [
        "model[('ASIAN','EXPORTERS')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFQa7E4ZApDx"
      },
      "outputs": [],
      "source": [
        "model[('ASIAN','EXPORTERS')]['FEAR']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5je1JUt9WOI"
      },
      "source": [
        "En caso de que no exista el trigrama con la tercera palabra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ReaBJK9QuE"
      },
      "outputs": [],
      "source": [
        "model[('ASIAN','EXPORTERS')]['UNIVERSITY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-k8iGkU9Z1d"
      },
      "source": [
        "En caso de que no exista el trigrama con la pareja de las primeras dos palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfP7tUasAgnq"
      },
      "outputs": [],
      "source": [
        "model[('STATE','OF')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVzu_hAZ9kY9"
      },
      "source": [
        "Ahora sí generamos todos los conteos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiU7UvFrA2u3"
      },
      "outputs": [],
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0)) # Valor por defecto si el trigrama no existe\n",
        "\n",
        "for w1, w2, w3 in tri_grams:\n",
        "    model[(w1, w2)][w3] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAOAo-3HAoEV"
      },
      "outputs": [],
      "source": [
        "print(model[('STATE','OF')]['THE'])\n",
        "print(model[('ASIAN','EXPORTERS')]['FEAR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_-a3WvhFv_3"
      },
      "source": [
        "Para entender cómo funciona la función siguiente, veamos que, en caso de no existir el trigrama podemos tratarlo como un booleano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiGZaDyzA8HI"
      },
      "source": [
        "Transformamos los conteos en probabilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4bQMmJ-A_MF"
      },
      "outputs": [],
      "source": [
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-zLHLCIBqxT"
      },
      "source": [
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58BTnIplBgHQ"
      },
      "outputs": [],
      "source": [
        "model[('given','that')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWgQFSahC4wz"
      },
      "outputs": [],
      "source": [
        "# Para un trigrama que no exista\n",
        "v = model[('DREAM','ABOUT')]  # diccionario con las posibles combinaciones de cada tercer palabra con las primeras dos\n",
        "print(v)\n",
        "if v:\n",
        "    print(\"True\")\n",
        "else:\n",
        "    print(\"False\")\n",
        "\n",
        "# Para un trigrama que sí exista\n",
        "v = model['ASIAN','EXPORTERS']\n",
        "print(v)\n",
        "if v:\n",
        "    print(\"True\")\n",
        "else:\n",
        "    print(\"False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcy4eXNCmva0"
      },
      "source": [
        "### Probemos el modelo en varias tareas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eeilZ1rBCut"
      },
      "source": [
        "Función que calcula la próxima palabra basada en dos palabras anteriores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvpS0Od25tV9"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(w1, w2):\n",
        "    next_word = model[w1, w2]  # El diccionario de la probabilidad de cada 3er palabra\n",
        "    if next_word:\n",
        "        predicted_word = max(next_word, key=next_word.get)  # Obtener la palabra más probable\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return \"SIN PREDICCIÓN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS3Gc1IBCBDX"
      },
      "outputs": [],
      "source": [
        "print(f\"Siguiente palabra para 'the'-'stock': '{predict_next_word('the', 'stock')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzt9tEEc5_gH"
      },
      "outputs": [],
      "source": [
        "text = \"the state of\"\n",
        "# text = \"ASIAN EXPORTERS FEAR DAMAGE\"\n",
        "# text = \"We are considering\"\n",
        "# text = \"Given the current\"\n",
        "\n",
        "w0 = text.split()[-2]\n",
        "w1 = text.split()[-1]\n",
        "print(f\"Siguiente palabra para '{w0}'-'{w1}': '{predict_next_word(text.split()[-2], text.split()[-1])}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH9ItY4vE3y9"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "punctuations = list(string.punctuation)\n",
        "\n",
        "def evaluar(text,model):\n",
        "    tokenized_text = [t for t in nltk.word_tokenize(text) if t not in punctuations]\n",
        "    aciertos = 0\n",
        "    for j,t in enumerate(tokenized_text):\n",
        "        if j != 0 and j<len(tokenized_text)-1:\n",
        "            previous_word = tokenized_text[j-1]\n",
        "            this_word = t\n",
        "            next_word = tokenized_text[j+1]\n",
        "            print(f\"Predicción para '{previous_word}'-'{this_word}': '{predict_next_word(previous_word, this_word)}'. Debería ser: '{next_word}'\")\n",
        "            if predict_next_word(previous_word, this_word) == next_word:\n",
        "                aciertos += 1\n",
        "    print(f\"\\nAciertos: {aciertos}/{len(tokenized_text)-2} = {round(100*aciertos/(len(tokenized_text)-2),2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-zbNkvZYb4l"
      },
      "source": [
        "Probemos con un texto pequeño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9QNbZayCw3W"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"The Ottomans kept them as guardians of the strategic Via Maris and Damascus–Jerusalem highways and rewarded them with tax farms\"\n",
        "\n",
        "evaluar(text,model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssv7khWjYfAq"
      },
      "source": [
        "Probemos con un texto más grande:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfClxTslJCOh"
      },
      "source": [
        "---\n",
        "\n",
        "Norwich Market (also known as Norwich Provision Market) is an outdoor market consisting of around 200 stalls in central Norwich, England. Founded in the latter part of the 11th century to supply Norman merchants and settlers moving to the area following the Norman conquest of England, it replaced an earlier market a short distance away. It has been in operation on the present site for over 900 years.\n",
        "\n",
        "By the 14th century, Norwich was one of the largest and most prosperous cities in England, and Norwich Market was a major trading hub. Control of, and income from, the market was ceded by the monarchy to the city of Norwich in 1341, from which time it provided a significant source of income for the local council. Freed from royal control, the market was reorganised to benefit the city as much as possible. Norwich and the surrounding region were devastated by plague and famine in the latter half of the 14th century, with the population falling by over 50%. Following the plague years, Norwich came under the control of local merchants and the economy was rebuilt. In the early 15th century, a Guildhall was built next to the market to serve as a centre for local government and law enforcement. The largest surviving mediaeval civic building in Britain outside London, it remained the seat of local government until 1938 and in use as a law court until 1985.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_pOLWSUFKKZ"
      },
      "outputs": [],
      "source": [
        "text = '''Norwich Market (also known as Norwich Provision Market) is an outdoor market consisting of around 200 stalls in central Norwich, England. Founded in the latter part of the 11th century to supply Norman merchants and settlers moving to the area following the Norman conquest of England, it replaced an earlier market a short distance away. It has been in operation on the present site for over 900 years.\n",
        "\n",
        "By the 14th century, Norwich was one of the largest and most prosperous cities in England, and Norwich Market was a major trading hub. Control of, and income from, the market was ceded by the monarchy to the city of Norwich in 1341, from which time it provided a significant source of income for the local council. Freed from royal control, the market was reorganised to benefit the city as much as possible. Norwich and the surrounding region were devastated by plague and famine in the latter half of the 14th century, with the population falling by over 50%. Following the plague years, Norwich came under the control of local merchants and the economy was rebuilt. In the early 15th century, a Guildhall was built next to the market to serve as a centre for local government and law enforcement. The largest surviving mediaeval civic building in Britain outside London, it remained the seat of local government until 1938 and in use as a law court until 1985. '''\n",
        "\n",
        "evaluar(text,model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Q0YFOwYjPC"
      },
      "source": [
        "Probemos con un texto del mismo corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA1T48lFUGQy"
      },
      "outputs": [],
      "source": [
        "text = reuters.raw('test/14826')\n",
        "# text = reuters.raw('test/14828')\n",
        "\n",
        "evaluar(text,model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TQwr3G3EIDG"
      },
      "source": [
        "⭕ Preguntas para reflexionar:\n",
        "\n",
        "* ¿Qué efecto tiene el tipo de corpus en la construcción y desempeño del modelo?\n",
        "* ¿Qué podríamos hacer para incrementar los aciertos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNMmY2D8F2qX"
      },
      "source": [
        "⭕ Realicemos una nube de palabras con los trigramas, ¿con o sin stopwords?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxPbDqOUGHY3"
      },
      "outputs": [],
      "source": [
        "!pip install -qq wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL2_lpwtLyN1"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tgs = [f\"{w1}_{w2}_{w3}\".lower() for w1,w2,w3 in tri_grams]\n",
        "\n",
        "wc = WordCloud(background_color='white')\n",
        "wc.generate(\" \".join(tgs))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBlXKt1m2co"
      },
      "source": [
        "Ahora, generemos automáticamente algunas palabras con este modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_JrxwYcnHHy"
      },
      "outputs": [],
      "source": [
        "vocab = list(set(list(reuters.words()))) # El vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1VY1XSvkRn-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "word_pair = list(model.keys())[94]\n",
        "\n",
        "# random_idx = np.random.randint(0, len(vocab))\n",
        "# word_pair = list(model.keys())[random_idx]\n",
        "\n",
        "print(word_pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvkxy7FwkxD-"
      },
      "outputs": [],
      "source": [
        "total_len = 25  # longitud del texto generado\n",
        "\n",
        "pos = 0\n",
        "print(word_pair[0],word_pair[1],end=' ')\n",
        "while pos<total_len:\n",
        "    next_word = predict_next_word(word_pair[0], word_pair[1])\n",
        "    print(next_word, end=' ')\n",
        "    word_pair = (word_pair[1], next_word)\n",
        "    pos += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGr-zjMjn6Xi"
      },
      "source": [
        "⭕ Preguntas:\n",
        "\n",
        "* ¿Qué tanto sentido tiene el texto?\n",
        "* ¿Cómo podemos evaluar estas tareas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-IzDDqpSkxv"
      },
      "source": [
        "## Unigramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5uroSPzTE5S"
      },
      "source": [
        "⭕ ¿Cómo serían las predicciones con un modelo de unigramas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqRTB2g7TS2S"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "conteos = Counter(list(reuters.words()))\n",
        "print(conteos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCIywopwSosh"
      },
      "source": [
        "## Generalizaciones de $n$-gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlTZyWxdSuJH"
      },
      "source": [
        "### Skipgrams\n",
        "\n",
        "Un $k$-skip-$n$-gram es una subsecuencia de longitud $n$ en la que los tokens aparecen a una distancia $k$ como máximo entre sí.\n",
        "\n",
        "\n",
        "https://www.nltk.org/api/nltk.util.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3DN-utBSwkd"
      },
      "outputs": [],
      "source": [
        "from nltk.util import skipgrams\n",
        "\n",
        "text = \"The rain in Spain falls mainly on the plain\"\n",
        "\n",
        "tokenized_text = [t for t in nltk.word_tokenize(text) if t not in punctuations]\n",
        "\n",
        "skip_grams = list(skipgrams(tokenized_text, 2, 1))\n",
        "print(skip_grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAFAeIVeJy_H"
      },
      "source": [
        "### Syntactic n-grams\n",
        "\n",
        "https://www.cic.ipn.mx/~sidorov/#sn-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkCz214cM1A0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}