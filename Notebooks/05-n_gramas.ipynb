{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDQDbYzeqA3D9j6mOmVaJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Notebooks/05-n_gramas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelos de lenguaje: $N$-gramas"
      ],
      "metadata": {
        "id": "xBW3_NymG21F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el corpus Reuters para entrenar un primer modelo de lenguaje. Este modelo lo usaremos para predicir la siguiente palabra en una secuencia de palabras dada.\n",
        "\n",
        "El Corpus Reuters contiene 10,788 documentos de noticias con un total de 1.3 millones de palabras. Los documentos se han clasificado en 90 temas y agrupado en dos conjuntos, *train* y *test*.\n",
        "\n",
        "* Corpus en NLTK: https://www.nltk.org/howto/corpus.html"
      ],
      "metadata": {
        "id": "oVrmwiUaFEvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1kvdbFQF9G1LTmcTVEPPy57nJRTuT2Kkt"
      ],
      "metadata": {
        "id": "6lprx7JUagw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de 3-gramas"
      ],
      "metadata": {
        "id": "NicPqSW5Sgcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos un poco el corpus."
      ],
      "metadata": {
        "id": "-b8q9JM1FVA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('wikipedia_batch_001_cleantext.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "VmrPMWZtbhSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    Limpia un texto removiendo dígitos, puntuaciones, caracteres especiales,\n",
        "    letras griegas, 'px', y otros elementos no deseados.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto a limpiar\n",
        "\n",
        "    Returns:\n",
        "        str: Texto limpio\n",
        "    \"\"\"\n",
        "\n",
        "    # Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Remover entidades HTML como &nbsp;\n",
        "    texto = re.sub(r'&[a-z]+;', ' ', texto)\n",
        "\n",
        "    # Remover 'px' (pixeles)\n",
        "    texto = re.sub(r'\\d*\\.?\\d+\\s*px', ' ', texto)\n",
        "    texto = re.sub(r'px', ' ', texto)\n",
        "\n",
        "    # Remover letras griegas (alfabeto griego completo)\n",
        "    # Rango Unicode para letras griegas: U+0370 a U+03FF\n",
        "    texto = ''.join(char if not ('\\u0370' <= char <= '\\u03FF') else ' ' for char in texto)\n",
        "\n",
        "    # Remover números (enteros y decimales)\n",
        "    texto = re.sub(r'\\d+\\.?\\d*', ' ', texto)\n",
        "\n",
        "    # Remover símbolos matemáticos y científicos comunes\n",
        "    texto = re.sub(r'[°±×÷√∞≈≠≤≥]', ' ', texto)\n",
        "\n",
        "    # Remover puntuación y caracteres especiales\n",
        "    # Mantener solo letras, espacios y algunos caracteres básicos\n",
        "    texto = re.sub(r'[^\\w\\sáéíóúñüàèìòùâêîôûäëïöü]', ' ', texto)\n",
        "\n",
        "    # Remover guiones bajos\n",
        "    texto = re.sub(r'_+', ' ', texto)\n",
        "\n",
        "    # Remover caracteres de control y formato\n",
        "    texto = ''.join(char for char in texto if unicodedata.category(char)[0] != 'C')\n",
        "\n",
        "    # Normalizar espacios múltiples a uno solo\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "4d5y0UBgc-SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean text'] = df['clean text'].apply(limpiar_texto)\n",
        "df"
      ],
      "metadata": {
        "id": "gryLvQ3ycOd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df['clean text'].sample(2):\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "iYSqH5BZUB62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear el modelo de lenguaje"
      ],
      "metadata": {
        "id": "l4Z42F4S3ALi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def crear_modelo_trigrama(texto):\n",
        "    \"\"\"\n",
        "    Crea un modelo de trigramas desde un texto.\n",
        "\n",
        "    texto: String con el texto de entrenamiento\n",
        "\n",
        "    Returns:\n",
        "        Modelo (diccionario de probabilidades)\n",
        "    \"\"\"\n",
        "    # Tokenizar\n",
        "    palabras = nltk.word_tokenize(texto.lower(), language='spanish')\n",
        "\n",
        "    # Crear trigramas\n",
        "    trigramas = []\n",
        "    for i in range(len(palabras) - 2):\n",
        "        trigramas.append((palabras[i], palabras[i+1], palabras[i+2]))\n",
        "\n",
        "    # Alternativamente, crear trigramas con nltk\n",
        "    # trigramas = list(trigrams(palabras))\n",
        "\n",
        "    # Contar\n",
        "    conteos = {}\n",
        "    for w1, w2, w3 in trigramas:\n",
        "        if (w1, w2) not in conteos:\n",
        "            conteos[(w1, w2)] = {}\n",
        "        if w3 not in conteos[(w1, w2)]:\n",
        "            conteos[(w1, w2)][w3] = 0\n",
        "        conteos[(w1, w2)][w3] += 1\n",
        "\n",
        "    # Convertir a probabilidades\n",
        "    modelo = {}\n",
        "    for par, terceras in conteos.items():\n",
        "        total = sum(terceras.values())\n",
        "        modelo[par] = {palabra: cuenta/total for palabra, cuenta in terceras.items()}\n",
        "\n",
        "    return modelo"
      ],
      "metadata": {
        "id": "0R1_nHEfeVyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos con una parte del corpus"
      ],
      "metadata": {
        "id": "1QGzpwS4gcgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\\n\".join(df.sample(1000)['clean text'].values)"
      ],
      "metadata": {
        "id": "oZRqIBABfCQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos un texto"
      ],
      "metadata": {
        "id": "t-b1cc3Jzfen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"la universidad de sonora en hermosillo hará huelga\"\n",
        "\n",
        "corpus += \"\\n\" + texto"
      ],
      "metadata": {
        "id": "9EH3TJ9ezfWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar modelo\n",
        "modelo = crear_modelo_trigrama(corpus)\n",
        "\n",
        "print(f\"\\nModelo entrenado con {len(modelo)} pares de palabras únicos\")"
      ],
      "metadata": {
        "id": "yazMgKBAe-kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo"
      ],
      "metadata": {
        "id": "S-bcYFmVggIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_siguiente(palabra1, palabra2, modelo):\n",
        "    \"\"\"\n",
        "    Predice la siguiente palabra más probable.\n",
        "\n",
        "    Args:\n",
        "        palabra1: Primera palabra del contexto\n",
        "        palabra2: Segunda palabra del contexto\n",
        "        modelo: Diccionario con probabilidades\n",
        "\n",
        "    Returns:\n",
        "        La palabra más probable, o None si no hay predicción\n",
        "    \"\"\"\n",
        "    par = (palabra1, palabra2)\n",
        "\n",
        "    # Verificar si conocemos este par de palabras\n",
        "    if par not in modelo:\n",
        "        return None\n",
        "\n",
        "    # Obtener la palabra con mayor probabilidad\n",
        "    probabilidades = modelo[par]\n",
        "    palabra_mas_probable = max(probabilidades, key=probabilidades.get)\n",
        "\n",
        "    return palabra_mas_probable\n",
        "\n",
        "def generar_texto(palabra_inicial1, palabra_inicial2, modelo, longitud=10):\n",
        "    \"\"\"\n",
        "    Genera texto palabra por palabra usando el modelo.\n",
        "\n",
        "    Args:\n",
        "        palabra_inicial1: Primera palabra para empezar\n",
        "        palabra_inicial2: Segunda palabra para empezar\n",
        "        modelo: Modelo de n-gramas\n",
        "        longitud: Cuántas palabras generar\n",
        "\n",
        "    Returns:\n",
        "        Texto generado como string\n",
        "    \"\"\"\n",
        "    # Empezar con las dos palabras iniciales\n",
        "    texto_generado = [palabra_inicial1, palabra_inicial2]\n",
        "\n",
        "    for _ in range(longitud):\n",
        "        # Usar las últimas dos palabras para predecir la siguiente\n",
        "        w1 = texto_generado[-2]\n",
        "        w2 = texto_generado[-1]\n",
        "\n",
        "        siguiente = predecir_siguiente(w1, w2, modelo)\n",
        "\n",
        "        # Si no hay predicción, terminar\n",
        "        if siguiente is None:\n",
        "            break\n",
        "\n",
        "        texto_generado.append(siguiente)\n",
        "\n",
        "    return ' '.join(texto_generado)"
      ],
      "metadata": {
        "id": "sEaMhx4vf44U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generar texto con el modelo"
      ],
      "metadata": {
        "id": "__bfIyMkVf5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras de inicio:\n",
        "inicio1, inicio2 = \"las\", \"primeras\"\n",
        "# inicio1, inicio2 = \"una\", \"investigación\"\n",
        "# inicio1, inicio2 = \"de\", \"sonora\"\n",
        "\n",
        "longitud_maxima = 20\n",
        "\n",
        "texto_generado = generar_texto(inicio1, inicio2, modelo, longitud=longitud_maxima)\n",
        "print(f\"Comenzando con '{inicio1} {inicio2}':\\n\\t{texto_generado}\")"
      ],
      "metadata": {
        "id": "JG1KHR0dfAvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelo[('las','primeras')]\n",
        "modelo[('una','investigación')]"
      ],
      "metadata": {
        "id": "K921Y9pMg3Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭕ Estrategias de mejora"
      ],
      "metadata": {
        "id": "fBxG0XsQhyJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quitar o dejar signos de puntuación.\n",
        "* Tokens de inicio y fin de texto\n",
        "* Vocabulario cerrado $<$UNK$>$\n",
        "* ¿Cómo serían las predicciones con un modelo de unigramas?\n",
        "* Modificar parámetros de generación de texto"
      ],
      "metadata": {
        "id": "wFUh7t8P-MsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estrategias de Muestreo para Generación de Texto\n",
        "\n",
        "Hasta ahora siempre elegimos la palabra **más probable**, pero esto hace que el texto sea repetitivo y predecible. Existen otras estrategias:\n",
        "\n",
        "#### 1. **Top-k Sampling**\n",
        "En lugar de elegir siempre la más probable, seleccionamos aleatoriamente entre las **k palabras más probables**.\n",
        "\n",
        "**Ejemplo:** Si k=3 y tenemos:\n",
        "- \"café\" (50%) ✅\n",
        "- \"té\" (30%) ✅\n",
        "- \"chocolate\" (15%) ✅\n",
        "- \"agua\" (5%) ❌\n",
        "\n",
        "Elegimos aleatoriamente solo entre café, té y chocolate (ignoramos \"agua\").\n",
        "\n",
        "**Efecto:** Más variedad, pero controlada. Con k pequeño es conservador, con k grande es más creativo.\n",
        "\n",
        "#### 2. **Top-p Sampling**\n",
        "Seleccionamos entre las palabras cuyas probabilidades **suman p**.\n",
        "\n",
        "**Ejemplo:** Con p=0.9:\n",
        "- \"café\" (50%) ✅\n",
        "- \"té\" (30%) ✅  \n",
        "- \"chocolate\" (15%) ✅ $→$ Total: 95%\n",
        "- \"agua\" (5%) ❌ (ya superamos 90%)\n",
        "\n",
        "**Efecto:** Se adapta al contexto. Si hay una palabra muy probable (95%), solo considera esa. Si hay varias opciones, considera más palabras.\n"
      ],
      "metadata": {
        "id": "k_PDR7iwTiaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "HxPbDqOUGHY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import trigrams, bigrams\n",
        "\n",
        "texto = \" \".join(df['clean text'].sample(1000).values)\n",
        "palabras = nltk.word_tokenize(texto.lower(), language='spanish')\n",
        "trigramas = list(trigrams(palabras))\n",
        "bigramas = list(bigrams(palabras))"
      ],
      "metadata": {
        "id": "Wbc-rsyZVw6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tgs = [f\"{w1}_{w2}_{w3}\".lower() for w1,w2,w3 in trigramas]\n",
        "bgs = [f\"{w1}_{w2}\".lower() for w1,w2 in bigramas]\n",
        "\n",
        "wc = WordCloud(background_color='white')\n",
        "# wc.generate(\" \".join(tgs))\n",
        "wc.generate(\" \".join(bgs))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dL2_lpwtLyN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizaciones de $n$-gramas"
      ],
      "metadata": {
        "id": "NCIywopwSosh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skipgrams\n",
        "\n",
        "Un $k$-skip-$n$-gram es una subsecuencia de longitud $n$ en la que los tokens aparecen a una distancia $k$ como máximo entre sí.\n",
        "\n",
        "\n",
        "https://www.nltk.org/api/nltk.util.html"
      ],
      "metadata": {
        "id": "nlTZyWxdSuJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import skipgrams\n",
        "from string import punctuation\n",
        "\n",
        "punctuations = list(punctuation)\n",
        "\n",
        "text = \"The rain in Spain falls mainly on the plain\"\n",
        "\n",
        "tokenized_text = [t for t in nltk.word_tokenize(text) if t not in punctuations]\n",
        "\n",
        "skip_grams = list(skipgrams(tokenized_text, 2, 1))\n",
        "print(skip_grams)"
      ],
      "metadata": {
        "id": "R3DN-utBSwkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkCz214cM1A0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}