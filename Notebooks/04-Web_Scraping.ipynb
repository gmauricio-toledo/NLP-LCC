{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2auNrte0mg/jMPGw5U8N/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Notebooks/04-Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Introducción al Web Scraping</h1>\n",
        "\n",
        "## ¿Qué es Web Scraping?\n",
        "\n",
        "El **web scraping** es una técnica para extraer información de sitios web de forma automatizada. En lugar de copiar y pegar manualmente datos de una página, escribimos código que lo hace por nosotros.\n",
        "\n",
        "## ¿Para qué sirve?\n",
        "\n",
        "- **Análisis de precios**: Comparar precios de productos en diferentes tiendas\n",
        "- **Recopilación de noticias**: Agregar artículos de múltiples fuentes\n",
        "- **Investigación**: Recolectar datos para estudios académicos\n",
        "- **Datos para Machine Learning**: Crear datasets personalizados\n",
        "\n",
        "---\n",
        "\n",
        "## Consideraciones Legales y Éticas\n",
        "\n",
        "Antes de hacer scraping, ten en cuenta:\n",
        "\n",
        "### Buenas prácticas:\n",
        "- Revisa el archivo `robots.txt` del sitio (ejemplo: `sitio.com/robots.txt`)\n",
        "- Respeta los Términos de Servicio del sitio web\n",
        "- No sobrecargues el servidor: añade delays entre peticiones\n",
        "- Usa los datos de forma responsable: respeta la privacidad\n",
        "\n",
        "### Evita:\n",
        "- Scraping de datos personales sensibles\n",
        "- Sobrecargar servidores con miles de peticiones simultáneas\n",
        "- Usar los datos para propósitos ilegales o no éticos\n",
        "- Ignorar las restricciones explícitas del sitio\n",
        "\n",
        "**Importante**: Si el sitio ofrece una [API](en.wikipedia.org/wiki/API), úsala en lugar de hacer scraping.\n",
        "\n",
        "---\n",
        "\n",
        "## Herramientas que Usaremos\n",
        "\n",
        "Para este tutorial trabajaremos con:\n",
        "\n",
        "1. **requests** - Para hacer peticiones HTTP y obtener el HTML\n",
        "2. **BeautifulSoup** - Para analizar y extraer datos del HTML\n",
        "\n"
      ],
      "metadata": {
        "id": "0dkhi_cpINMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero necesitamos instalar las librerías que usaremos:\n",
        "- **requests**: para hacer peticiones HTTP\n",
        "- **beautifulsoup4**: para analizar HTML\n",
        "- **lxml**: parser para BeautifulSoup (más rápido)"
      ],
      "metadata": {
        "id": "zcagEAt_Kuq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ffx26A9IKSe"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"requests versión:\", requests.__version__)"
      ],
      "metadata": {
        "id": "w_s5pIWMKyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "638kcOXhg0kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear nuestro primer scraper extrayendo información de una página web sencilla."
      ],
      "metadata": {
        "id": "O1KJrbqZK_fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer una petición a una página web\n",
        "url = \"https://www.unison.mx\"\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "GYmbNElEK_TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar que la petición fue exitosa"
      ],
      "metadata": {
        "id": "6eoWvHuKTgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Status code: {response.status_code}\")\n",
        "print(f\"Content type: {response.headers['Content-Type']}\")"
      ],
      "metadata": {
        "id": "SSY5KE2WK5_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Status code 200 significa que todo salió bien. Códigos de estado HTTP comunes:\n",
        "- **200**: OK - Todo bien\n",
        "- **404**: Not Found - Página no encontrada\n",
        "- **403**: Forbidden - Acceso denegado\n",
        "- **500**: Server Error - Error del servidor"
      ],
      "metadata": {
        "id": "t7jCoA1aTmD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionar el HTML recibido"
      ],
      "metadata": {
        "id": "EaF7PrSKTvOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Primeros 600 caracteres del HTML:\")\n",
        "print(\"-\" * 50)\n",
        "print(response.text[:600])"
      ],
      "metadata": {
        "id": "lGuJQqevTmX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora usemos BeautifulSoup para analizar el HTML y extraer información:"
      ],
      "metadata": {
        "id": "2XST5pFzT2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "# Extraer el título de la página\n",
        "titulo = soup.find('title')\n",
        "\n",
        "print(\"Título de la página:\")\n",
        "print(titulo.text)\n",
        "\n",
        "# También podemos extraer el encabezado principal\n",
        "h1 = soup.find('h1')\n",
        "if h1:\n",
        "    print(f\"\\nEncabezado H1: {h1.text}\")"
      ],
      "metadata": {
        "id": "pNKgCmHITx7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "id": "--r5Zas5T96h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que sabemos cómo obtener el HTML, el objetivo principal es **extraer el texto limpio** para usarlo en tareas de procesamiento de lenguaje natural."
      ],
      "metadata": {
        "id": "aWSIxU7_ePAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usemos un artículo de ejemplo\n",
        "# url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "url = \"https://www.unison.mx/nota/?idnoti=38125\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "# FORMA INCORRECTA: Obtener TODO el texto\n",
        "texto_sucio = soup.get_text()\n",
        "print(\"TEXTO SUCIO (primeros 500 caracteres):\")\n",
        "print(texto_sucio[:500])"
      ],
      "metadata": {
        "id": "wE7f_XplUdgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_sucio"
      ],
      "metadata": {
        "id": "rWSjI7PiebUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "TLw4YNazgvmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`find_all`](https://beautiful-soup-4.readthedocs.io/en/latest/#find-all)"
      ],
      "metadata": {
        "id": "SA-GsHkdg-D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://books.toscrape.com/catalogue/category/books/travel_2/index.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "# Obtener todos los enlaces a libros\n",
        "libros = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "datos_libros = []\n",
        "\n",
        "for libro in libros[:5]:  # Solo los primeros 5 para el ejemplo\n",
        "    # Extraer título\n",
        "    titulo = libro.find('h3').find('a')['title']\n",
        "\n",
        "    # Extraer precio\n",
        "    precio = libro.find('p', class_='price_color').text\n",
        "\n",
        "    # Extraer disponibilidad\n",
        "    disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
        "\n",
        "    datos_libros.append({\n",
        "        'titulo': titulo,\n",
        "        'precio': precio,\n",
        "        'disponibilidad': disponibilidad\n",
        "    })\n",
        "\n",
        "    print(f\"{titulo}\")\n",
        "    print(f\"\\t{precio} - {disponibilidad}\\n\")\n",
        "\n",
        "print(f\"Se extrajeron {len(datos_libros)} libros\")"
      ],
      "metadata": {
        "id": "d05_Sgq7fHV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "GJhJwmv7jq0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "id": "0XDDS31sxFcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "USER_AGENT = \"TutorialWebScraping\"  # <tarjeta de presentación> cuando haces peticiones al servidor\n",
        "en_wiki = wikipediaapi.Wikipedia(USER_AGENT, language='en')\n",
        "es_wiki = wikipediaapi.Wikipedia(USER_AGENT, language='es')\n",
        "\n",
        "tema = ... # Pon el tema que prefieras\n",
        "page = en_wiki.page(tema)\n",
        "if page.exists():\n",
        "    text = page.text  # Puedes usar page.summary si solo quieres el resumen\n",
        "\n",
        "print(text)\n",
        "\n",
        "# #--- También podríamos buscar en español ---\n",
        "# page = es_wiki.page(tema)\n",
        "# if page.exists():\n",
        "#     text = page.text"
      ],
      "metadata": {
        "id": "5lTnyEbijsVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "OiyAMUhnxbyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el texto extraido"
      ],
      "metadata": {
        "id": "FMy-ov__jXb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos el texto extraido en un archivo de texto"
      ],
      "metadata": {
        "id": "2KlSFASxyLVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"texto_extraido.txt\"\n",
        "with open(fname, 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "Q_QMvJjLfqJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad en clase"
      ],
      "metadata": {
        "id": "vJA_Pmm6zcf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Extrae el contenido de una página de tu preferencia (puede ser académico, noticia, etc.). Realiza una limpieza básica y guarda el texto en un archivo de texto con el nombre:\n",
        "\n",
        "`apellido_nombre_1.txt`\n",
        "\n",
        "2. Extrae el contenido de un artículo de wikipedia de tu preferencia. Realiza una limpieza básica y guarda el texto en un archivo de texto con el nombre:\n",
        "\n",
        "`apellido_nombre_2.txt`"
      ],
      "metadata": {
        "id": "JrH_PZaB9gdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrWCiHUSMxGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}