{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-LCC/blob/main/Notebooks/06-Vectores_sem%C3%A1nticos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Semántica Vectorial</h1>\n",
        "\n",
        "En esta notebook usaremos dos módelos de semántica vectorial para diversas tareas de NLP. Los modelos que usaremos son:\n",
        "\n",
        "* Bag of Words (BoW). [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "* Term Frequency - Inverse Document Frequency (TF-IDF). [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "\n",
        "Usando estos dos modelos realicermos tareas como:\n",
        "\n",
        "* Vecinos más cercanos\n",
        "* Information Retrieval\n",
        "* Segmentación\n",
        "* Clasificación\n"
      ],
      "metadata": {
        "id": "flcPOaSE3dZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "b7x0zZC6NSdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbd33fd-b0a0-46b6-adf3-593eb0193f11"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus 0: Detección de SPAM"
      ],
      "metadata": {
        "id": "u613pEDMA0gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leer el corpus"
      ],
      "metadata": {
        "id": "OrUXDmGpSfH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-pMLSTkJ3ZPCKQU8oXA3uI-swvXw7DmJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq0P-lmyA0U9",
        "outputId": "df227bfb-3e25-4be7-dc72-cb21acee13d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-pMLSTkJ3ZPCKQU8oXA3uI-swvXw7DmJ\n",
            "To: /content/Spam_SMS.csv\n",
            "\r  0% 0.00/487k [00:00<?, ?B/s]\r100% 487k/487k [00:00<00:00, 27.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Spam_SMS.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "HoWfqj-bBZCq",
        "outputId": "2302616c-dd32-4fb8-bfe5-c38dff645018"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Class                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5570   ham               Will ü b going to esplanade fr home?\n",
              "5571   ham  Pity, * was in mood for that. So...any other s...\n",
              "5572   ham  The guy did some bitching but I acted like i'd...\n",
              "5573   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5574 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-909e68c0-6c61-4e64-9879-0861077dae59\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5572</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5573</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5574 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-909e68c0-6c61-4e64-9879-0861077dae59')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-909e68c0-6c61-4e64-9879-0861077dae59 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-909e68c0-6c61-4e64-9879-0861077dae59');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_3143f6be-86b6-485f-af90-a74928fea31c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3143f6be-86b6-485f-af90-a74928fea31c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5574,\n  \"fields\": [\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5159,\n        \"samples\": [\n          \"\\\"HEY KATE, HOPE UR OK... WILL GIVE U A BUZ WEDLUNCH. GO OUTSOMEWHERE 4 ADRINK IN TOWN..CUD GO 2WATERSHD 4 A BIT? PPL FROMWRK WILL BTHERE. LOVE PETEXXX.\\\"\",\n          \"Nobody names their penis a girls name this story doesn't add up at all\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in df.sample(10).index.to_list():\n",
        "    print(df.loc[idx,'Message'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5X1tKdBlLP",
        "outputId": "746b2494-43cd-4cda-ca6a-b179f14acc9e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why must we sit around and wait for summer days to celebrate. Such a magical sight when the worlds dressed in white. Oooooh let there be snow.\n",
            "I guess it is useless calling u 4 something important.\n",
            "Well, I was about to give up cos they all said no they didn‘t do one nighters. I persevered and found one but it is very cheap so i apologise in advance. It is just somewhere to sleep isnt it?\n",
            "How come it takes so little time for a child who is afraid of the dark to become a teenager who wants to stay out all night?\n",
            "If india win or level series means this is record:)\n",
            "Yes. Please leave at  &lt;#&gt; . So that at  &lt;#&gt;  we can leave\n",
            "I'm leaving my house now...\n",
            ":)\n",
            "URGENT! Your Mobile No was awarded a £2,000 Bonus Caller Prize on 1/08/03! This is our 2nd attempt to contact YOU! Call 0871-4719-523 BOX95QU BT National Rate\n",
            "Was it something u ate?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza del texto"
      ],
      "metadata": {
        "id": "0SrFfgkzSc6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "    texto = re.sub(r'\\d+', ' ', texto)\n",
        "    return texto\n",
        "\n",
        "df['texto limpio'] = df['Message'].apply(limpiar_texto)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "G4HbA345BiP7",
        "outputId": "3aefac40-f6c4-4027-c53a-a13ac3bd618d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Class                                            Message  \\\n",
              "0      ham  Go until jurong point, crazy.. Available only ...   \n",
              "1      ham                      Ok lar... Joking wif u oni...   \n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3      ham  U dun say so early hor... U c already then say...   \n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
              "...    ...                                                ...   \n",
              "5569  spam  This is the 2nd time we have tried 2 contact u...   \n",
              "5570   ham               Will ü b going to esplanade fr home?   \n",
              "5571   ham  Pity, * was in mood for that. So...any other s...   \n",
              "5572   ham  The guy did some bitching but I acted like i'd...   \n",
              "5573   ham                         Rofl. Its true to its name   \n",
              "\n",
              "                                           texto limpio  \n",
              "0     go until jurong point crazy available only in ...  \n",
              "1                               ok lar joking wif u oni  \n",
              "2     free entry in   a wkly comp to win fa cup fina...  \n",
              "3           u dun say so early hor u c already then say  \n",
              "4     nah i dont think he goes to usf he lives aroun...  \n",
              "...                                                 ...  \n",
              "5569  this is the  nd time we have tried   contact u...  \n",
              "5570                will ü b going to esplanade fr home  \n",
              "5571  pity  was in mood for that soany other suggest...  \n",
              "5572  the guy did some bitching but i acted like id ...  \n",
              "5573                          rofl its true to its name  \n",
              "\n",
              "[5574 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf4003b6-a4a5-432d-a43a-c892a7d4baa9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Message</th>\n",
              "      <th>texto limpio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point crazy available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in   a wkly comp to win fa cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>this is the  nd time we have tried   contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "      <td>will ü b going to esplanade fr home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>pity  was in mood for that soany other suggest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5572</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>the guy did some bitching but i acted like id ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5573</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>rofl its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5574 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf4003b6-a4a5-432d-a43a-c892a7d4baa9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf4003b6-a4a5-432d-a43a-c892a7d4baa9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf4003b6-a4a5-432d-a43a-c892a7d4baa9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_dd7c97fd-3634-4fb2-9e99-61f57c29ba16\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dd7c97fd-3634-4fb2-9e99-61f57c29ba16 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5574,\n  \"fields\": [\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5159,\n        \"samples\": [\n          \"\\\"HEY KATE, HOPE UR OK... WILL GIVE U A BUZ WEDLUNCH. GO OUTSOMEWHERE 4 ADRINK IN TOWN..CUD GO 2WATERSHD 4 A BIT? PPL FROMWRK WILL BTHERE. LOVE PETEXXX.\\\"\",\n          \"Nobody names their penis a girls name this story doesn't add up at all\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"texto limpio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5106,\n        \"samples\": [\n          \"nothing smsing u n xy lor sorry lor da guys neva c u in person but they sort of know u lor so u wan   meet them xy ask me   bring u along   our next meeting\",\n          \"gud mrng dear have a nice day\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Class'].values)\n",
        "print(y[:5])\n",
        "\n",
        "docs = df['texto limpio'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgd90udnBejN",
        "outputId": "5308f507-4d90-489c-d731-acf8fdbed1a4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## División train/test"
      ],
      "metadata": {
        "id": "MpKRqXSyT6JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(docs, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=12)"
      ],
      "metadata": {
        "id": "tRgUL3ExCx63"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracción de variables (vectorización)"
      ],
      "metadata": {
        "id": "2aGGNKpCT23d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "cv = CountVectorizer(stop_words=stopwords)\n",
        "cv.fit(X_train)\n",
        "\n",
        "X_train_bow = cv.transform(X_train).todense()\n",
        "X_train_bow = np.array(X_train_bow)\n",
        "X_test_bow = cv.transform(X_test).todense()\n",
        "X_test_bow = np.array(X_test_bow)"
      ],
      "metadata": {
        "id": "tcy-H_j1C3kP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa las dimensiones de las matrices BOW"
      ],
      "metadata": {
        "id": "91hixy9GTzFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bow.shape, X_test_bow.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmGB8GnKTv8d",
        "outputId": "3537b997-68ce-4fe8-96a6-e69dff099897"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4459, 7355), (1115, 7355))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la proporción de ceros"
      ],
      "metadata": {
        "id": "jWsdLrm4TzCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_zero_entries = np.count_nonzero(X_train_bow == 0)\n",
        "number_of_entries = X_train_bow.shape[0] * X_train_bow.shape[1]\n",
        "\n",
        "print(f\"Porcentaje de entradas cero: {number_of_zero_entries/number_of_entries}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTDxqJ8JUDJd",
        "outputId": "593bca8b-295e-4862-9927-2babd958bf5c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porcentaje de entradas cero: 0.9988902896379416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretabilidad de las variables"
      ],
      "metadata": {
        "id": "C3PTHQMsU6Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "id": "N73EYhGDUQvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento e inferencia"
      ],
      "metadata": {
        "id": "NzVztyxzUA77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train_bow, y_train)\n",
        "\n",
        "y_pred_train = svm.predict(X_train_bow)\n",
        "y_pred_test = svm.predict(X_test_bow)"
      ],
      "metadata": {
        "id": "MH0q2BonC6dQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "Bd7sJloMU81r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "print(f\"Accuracy train: {accuracy_score(y_train, y_pred_train)}\")\n",
        "print(f\"Accuracy test: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(f\"F1 train: {f1_score(y_train, y_pred_train)}\")\n",
        "print(f\"F1 test: {f1_score(y_test, y_pred_test)}\")\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix(y_test, y_pred_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELjzqEKeU_aJ",
        "outputId": "19d36705-d316-4609-dc4a-8105346c65b2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.9993272034088361\n",
            "Accuracy test: 0.9838565022421525\n",
            "F1 train: 0.9975103734439834\n",
            "F1 test: 0.9343065693430657\n",
            "Matriz de confusión:\n",
            "[[969   3]\n",
            " [ 15 128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acerca de la interpretabilidad"
      ],
      "metadata": {
        "id": "C23FXkkyVed6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=20)\n",
        "dt.fit(X_train_bow, y_train)\n",
        "\n",
        "y_pred_train = dt.predict(X_train_bow)\n",
        "y_pred_test = dt.predict(X_test_bow)\n",
        "\n",
        "print(f\"Accuracy train: {accuracy_score(y_train, y_pred_train)}\")\n",
        "print(f\"Accuracy test: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(f\"F1 train: {f1_score(y_train, y_pred_train)}\")\n",
        "print(f\"F1 test: {f1_score(y_test, y_pred_test)}\")\n",
        "print(f\"Matriz de confusión:\\n{confusion_matrix(y_test, y_pred_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRklbN59VWUu",
        "outputId": "1f25028e-b558-406c-dbcf-0e722cf036c6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.9854227405247813\n",
            "Accuracy test: 0.9704035874439462\n",
            "F1 train: 0.9431321084864392\n",
            "F1 test: 0.8764044943820225\n",
            "Matriz de confusión:\n",
            "[[965   7]\n",
            " [ 26 117]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las palabras con las importancias más altas para la clasificación de la clase positiva (spam)"
      ],
      "metadata": {
        "id": "kljVWSLQWoJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_important_words_idxs = np.argsort(dt.feature_importances_)[::-1]\n",
        "most_important_words = [cv.get_feature_names_out()[idx] for idx in most_important_words_idxs]"
      ],
      "metadata": {
        "id": "DQuICd_ZWBu3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_important_words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-LkcoATWhxL",
        "outputId": "8ca547d1-a830-4a43-c9c6-bc4894afeff8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['call',\n",
              " 'txt',\n",
              " 'free',\n",
              " 'reply',\n",
              " 'text',\n",
              " 'ill',\n",
              " 'claim',\n",
              " 'pmsg',\n",
              " 'im',\n",
              " 'mobile']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus 1: Wikipedia"
      ],
      "metadata": {
        "id": "azUhLD1uMZpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_9XZLsaW8ASk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora probemos con otro corpus. Es una parte de un dump de wikipedia del 2006 ([información](https://www.cs.upc.edu/~nlp/wikicorpus/))."
      ],
      "metadata": {
        "id": "CU1UkHSt3aCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/gmauricio-toledo/NLP-MCD/main/data/spanish-wikipedia-dataframe.csv\"\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "guN3Df38qcdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocesamos y limpiamos el texto.\n",
        "\n",
        "⭕ ¿Qué estamos haciendo al texto?"
      ],
      "metadata": {
        "id": "H9Hi9jbpyUuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_raw = df['Texto'].tolist()\n",
        "docs = [re.sub(r'\\d+', ' ', doc) for doc in docs_raw]\n",
        "tokenized_docs = [word_tokenize(doc) for doc in docs]\n",
        "docs = [[token for token in doc if token not in nltk.corpus.stopwords.words('spanish')] for doc in tokenized_docs]\n",
        "docs = [' '.join(doc) for doc in docs]\n",
        "docs[:3]"
      ],
      "metadata": {
        "id": "WMj-PyHK6KTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo BOW"
      ],
      "metadata": {
        "id": "vvEb_zN_MyYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa cómo especificamos la lista de stopwords en español."
      ],
      "metadata": {
        "id": "0h1hewVvyjGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "cv = CountVectorizer(stop_words=stop_words, max_features=1000)\n",
        "X_bow = cv.fit_transform(docs)\n",
        "X_bow.shape"
      ],
      "metadata": {
        "id": "O4BFKBhM7s9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bow[:3,:7].todense()"
      ],
      "metadata": {
        "id": "NAYOhAPejOHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bow[140:143,750:756].todense()"
      ],
      "metadata": {
        "id": "BEigfAFRjg9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Qué tan *sparse* es la matriz?"
      ],
      "metadata": {
        "id": "r2dVGFnfrgjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ceros = np.where(X_bow.toarray()==0)[0].shape[0]\n",
        "total_entradas = (X_bow.toarray().shape[0]*X_bow.toarray().shape[1])\n",
        "\n",
        "print(f\"Número de entradas: {total_entradas}\")\n",
        "print(f\"Proporción de entradas cero: {round(100*num_ceros/total_entradas,2)} %\")"
      ],
      "metadata": {
        "id": "v_le-5Rkrf2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = cv.get_feature_names_out()"
      ],
      "metadata": {
        "id": "hmaakzysxyJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectores de documentos"
      ],
      "metadata": {
        "id": "F_FecHXYM0zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representaciones de documentos"
      ],
      "metadata": {
        "id": "2X1AzKRfxBXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectors = X_bow.toarray()"
      ],
      "metadata": {
        "id": "Sdq98skMxAIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionemos los vecinos más cercanos de ciertos documentos"
      ],
      "metadata": {
        "id": "CpkFu2_rbMH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn.fit(doc_vectors)"
      ],
      "metadata": {
        "id": "ezeqFvpnxXLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_number = 189\n",
        "# doc_number = np.random.randint(0, len(docs_raw))\n",
        "\n",
        "print(f\"Consulta:\\n\\t{docs_raw[doc_number]}\\n\")\n",
        "\n",
        "v_doc = doc_vectors[doc_number,:].reshape(-1,)\n",
        "nns = nn.kneighbors([v_doc])\n",
        "print(f\"Vecinos más cercanos: {[idx for idx in nns[1][0]]}\\n\")\n",
        "\n",
        "for idx,dist in zip(nns[1][0],nns[0][0]):\n",
        "    print(f\"Distancia: {round(dist,3)}\")\n",
        "    print(f\"{docs_raw[idx]}\\n\")"
      ],
      "metadata": {
        "id": "RYh7nB7Mxqco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Retrieval"
      ],
      "metadata": {
        "id": "j80f_0i-cjXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionemos los vecinos más cercanos de una query"
      ],
      "metadata": {
        "id": "lelQ6e8MbQZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"sello discográfico de artistas de pop\"\n",
        "\n",
        "query_vector = cv.transform([query]).toarray().reshape(-1,)\n",
        "print(query_vector)\n",
        "\n",
        "responses = nn.kneighbors([query_vector])\n",
        "for idx,dist in zip(responses[1][0],responses[0][0]):\n",
        "    print(f\"Distancia: {round(dist,3)}\")\n",
        "    print(f\"{docs_raw[idx]}\\n\")"
      ],
      "metadata": {
        "id": "mmHiDj0qbThw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grafiquemos la reducción de dimensionalidad 3d t-SNE\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "tsne = TSNE(n_components=3, metric='cosine')\n",
        "X_tsne = tsne.fit_transform(doc_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_tsne[:,0],\n",
        "    y=X_tsne[:,1],\n",
        "    z=X_tsne[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.75,\n",
        "        'color': 'black'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{docs_raw[j][:75]}\" for j in range(X_tsne.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Docs',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis =dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-bow-tsne3d-docs.html')"
      ],
      "metadata": {
        "id": "j2OMOMJvxx4Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering: Topic Modelling\n",
        "\n",
        "Clustericemos los documentos. Usemos un método basado en densidad, en lugar de uno de partición."
      ],
      "metadata": {
        "id": "EywlAKiUX1ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=3, metric='cosine')\n",
        "dbscan.fit(doc_vectors)\n",
        "num_doc_clusters = np.max(dbscan.labels_)+1\n",
        "print(f\"Hay {num_doc_clusters} clusters\")"
      ],
      "metadata": {
        "id": "Lg2aPErmYGrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "jUiJ_o9QE2wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title función para factorizar\n",
        "import math\n",
        "\n",
        "def factor_int(n):\n",
        "    val = math.ceil(math.sqrt(n))\n",
        "    val2 = int(n/val)\n",
        "    while val2 * val != float(n):\n",
        "        val -= 1\n",
        "        val2 = int(n/val)\n",
        "    return val, val2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cWPEj56HaRFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos los términos más frecuentes en cada cluster."
      ],
      "metadata": {
        "id": "UXgfgCC8zITk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "idxs_per_cluster = {j: np.where(dbscan.labels_==j)[0] for j in range(num_doc_clusters)}\n",
        "docs_per_cluster = {j: [docs[idx] for idx in idxs_per_cluster[j]] for j in idxs_per_cluster.keys()}\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", max_words=1000)\n",
        "\n",
        "w, h = factor_int(num_doc_clusters)\n",
        "fig, axs = plt.subplots(w, h, figsize=(6*w, 3*h))\n",
        "\n",
        "for j,ax in zip(idxs_per_cluster.keys(),axs.flatten()):\n",
        "    wc.generate(' '.join(docs_per_cluster[j]))\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Cluster {j}\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ysPfYvM0Z2aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectores de palabras"
      ],
      "metadata": {
        "id": "6_FnZQc4M3Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos las palabras:"
      ],
      "metadata": {
        "id": "4a0iiH-m6jKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "    idx = np.where(vocabulary==word)[0][0]\n",
        "    return X_bow[:, idx].toarray().flatten()"
      ],
      "metadata": {
        "id": "bpTc1Edw2XJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = [get_word_vector(word) for word in vocabulary]\n",
        "word_vectors = np.array(word_vectors)\n",
        "word_vectors.shape"
      ],
      "metadata": {
        "id": "b92R6nFa6nXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn.fit(word_vectors)"
      ],
      "metadata": {
        "id": "hfcgse0E6y_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos los vecinos más cercanos de las palabras: cine, equipo, guerra, música, mayores"
      ],
      "metadata": {
        "id": "ZvtZiisy68wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'guerra'\n",
        "v = get_word_vector(word)\n",
        "nns = nn.kneighbors([v])\n",
        "print(f\"Vecinos más cercanos: {[vocabulary[idx] for idx in nns[1][0]]}\")\n",
        "print(f\"Distancias: {[round(sim,3) for sim in nns[0][0]]}\")"
      ],
      "metadata": {
        "id": "p8ygy9qh63N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grafiquemos la reducción de dimensionalidad 3d t-SNE\n",
        "\n",
        "tsne = TSNE(n_components=3, metric='cosine')\n",
        "X_tsne = tsne.fit_transform(word_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_tsne[:,0],\n",
        "    y=X_tsne[:,1],\n",
        "    z=X_tsne[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.75,\n",
        "        'color': 'black'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{vocabulary[j]}\" for j in range(X_tsne.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Words',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis =dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-bow-tsne3d-words.html')"
      ],
      "metadata": {
        "id": "bO_Kh_BX65w5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq umap-learn\n",
        "\n",
        "import umap"
      ],
      "metadata": {
        "id": "YP2x6N1oWZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "\n",
        "Analicemos algunos clusters de palabras"
      ],
      "metadata": {
        "id": "IHhkW9AoM7Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 5\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
        "kmeans.fit(word_vectors)"
      ],
      "metadata": {
        "id": "IBWx39A79P4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(n_clusters):\n",
        "    print(f\"Cluster {j}:\")\n",
        "    print([vocabulary[idx] for idx in np.where(kmeans.labels_==j)[0]])"
      ],
      "metadata": {
        "id": "i-5YmojmAGrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analicemos los casos de *familia*, *campeón*"
      ],
      "metadata": {
        "id": "_vyUZ15eUsbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'campeón'\n",
        "v = get_word_vector(word)\n",
        "nns = nn.kneighbors([v])\n",
        "print(f\"Vecinos más cercanos: {[vocabulary[idx] for idx in nns[1][0]]}\")\n",
        "print(f\"Distancias: {[round(sim,3) for sim in nns[0][0]]}\")"
      ],
      "metadata": {
        "id": "5krkhnQEUfgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ ¿Por qué tenemos estos resultados que no corresponden a la gráfica?"
      ],
      "metadata": {
        "id": "5WXrK6NRM9Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "agglom = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine',linkage='average')\n",
        "agglom.fit(word_vectors)"
      ],
      "metadata": {
        "id": "A5glrXlqVuvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(n_clusters):\n",
        "    print(f\"Cluster {j}:\")\n",
        "    print([vocabulary[idx] for idx in np.where(agglom.labels_==j)[0]])"
      ],
      "metadata": {
        "id": "-g-FcKFHV_0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'abril'\n",
        "v = get_word_vector(word)\n",
        "nns = nn.kneighbors([v])\n",
        "print(f\"Vecinos más cercanos: {[vocabulary[idx] for idx in nns[1][0]]}\")\n",
        "print(f\"Distancias: {[round(sim,3) for sim in nns[0][0]]}\")"
      ],
      "metadata": {
        "id": "QBVZ0xadAub4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.35, min_samples=2, metric='cosine')\n",
        "dbscan.fit(word_vectors)"
      ],
      "metadata": {
        "id": "vY6Ncqh6WS93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in np.unique(dbscan.labels_):\n",
        "    print(f\"Cluster {j}:\")\n",
        "    print([vocabulary[idx] for idx in np.where(dbscan.labels_==j)[0]])"
      ],
      "metadata": {
        "id": "w_GeVTM8Wh3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo TF-IDF"
      ],
      "metadata": {
        "id": "hl7MP-sXNCFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "tfv = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
        "X_tfidf = tfv.fit_transform(docs)\n",
        "print(X_tfidf.shape)"
      ],
      "metadata": {
        "id": "f5kIZMolAtJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver la matriz, ¿es más sparse? Tenía el 94.78% de entradas en 0"
      ],
      "metadata": {
        "id": "m_drPj9NPOk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tota_entradas = X_tfidf.shape[0]*X_tfidf.shape[1]\n",
        "num_ceros = np.where(X_tfidf.toarray()==0)[0].shape[0]\n",
        "\n",
        "print(f\"Número de entradas: {total_entradas}\")\n",
        "print(f\"Proporción de entradas cero: {round(100*num_ceros/total_entradas,2)} %\")"
      ],
      "metadata": {
        "id": "s_cv3Ye9PNVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectores de palabras"
      ],
      "metadata": {
        "id": "LmwP1L1YNGG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = tfv.get_feature_names_out()\n",
        "\n",
        "def get_word_vector(word):\n",
        "    idx = np.where(vocabulary==word)[0][0]\n",
        "    return X_tfidf[:, idx].toarray().flatten()"
      ],
      "metadata": {
        "id": "X93kQn1KK3q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = [get_word_vector(word) for word in vocabulary]\n",
        "word_vectors = np.array(word_vectors)\n",
        "word_vectors.shape"
      ],
      "metadata": {
        "id": "xSf4WwnMNIBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn.fit(word_vectors)\n",
        "\n",
        "word = 'años'\n",
        "v = get_word_vector(word)\n",
        "nns = nn.kneighbors([v])\n",
        "print(f\"Vecinos más cercanos: {[vocabulary[idx] for idx in nns[1][0]]}\")\n",
        "print(f\"Distancias: {[round(sim,3) for sim in nns[0][0]]}\")"
      ],
      "metadata": {
        "id": "OU-eACAzNKwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grafiquemos la reducción de dimensionalidad 3d con t-SNE\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "tsne = TSNE(n_components=3,metric='cosine')\n",
        "X_tsne = tsne.fit_transform(word_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_tsne[:,0],\n",
        "    y=X_tsne[:,1],\n",
        "    z=X_tsne[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.75,\n",
        "        'color': 'black'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{vocabulary[j]}\" for j in range(X_tsne.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Words',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis =dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-tfidf-tsne3d-words.html')"
      ],
      "metadata": {
        "id": "a96oClijNPWo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectores de documentos"
      ],
      "metadata": {
        "id": "_VCe7pELPIh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectors = X_tfidf.toarray()"
      ],
      "metadata": {
        "id": "MjeD7Sb1PTqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn.fit(doc_vectors)"
      ],
      "metadata": {
        "id": "5h8EyQDNu36X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Retrieval"
      ],
      "metadata": {
        "id": "Vhyk1ortvfTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"sello discográfico de artistas de pop\"\n",
        "query = \"acontecimientos importantes en abril o nacido en abril\"\n",
        "\n",
        "query_vector = tfv.transform([query]).toarray().reshape(-1,)\n",
        "\n",
        "if np.sum(query_vector)==0:\n",
        "    print(\"Query no válida (OOV)\")\n",
        "else:\n",
        "    responses = nn.kneighbors([query_vector])\n",
        "    for idx,dist in zip(responses[1][0],responses[0][0]):\n",
        "        print(f\"Distancia: {round(dist,3)}\")\n",
        "        print(f\"{docs_raw[idx]}\\n\")"
      ],
      "metadata": {
        "id": "uAJUc0CbvBV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reducción de dimensionalidad 3d t-SNE\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "tsne = TSNE(n_components=3, metric='cosine')\n",
        "X_tsne = tsne.fit_transform(doc_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_tsne[:,0],\n",
        "    y=X_tsne[:,1],\n",
        "    z=X_tsne[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.75,\n",
        "        'color': 'black'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{docs_raw[j][:75]}\" for j in range(X_tsne.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Docs',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis =dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-tfidf-tsne3d-docs.html')"
      ],
      "metadata": {
        "id": "tFoJsdzrvFP4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq umap-learn"
      ],
      "metadata": {
        "id": "gBF9ulo-Id0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reducción de dimensionalidad 3d UMAP\n",
        "\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "umap = UMAP(n_components=3, metric='cosine')\n",
        "X_umap = umap.fit_transform(doc_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_umap[:,0],\n",
        "    y=X_umap[:,1],\n",
        "    z=X_umap[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.75,\n",
        "        'color': 'black'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{docs_raw[j][:75]}\" for j in range(X_umap.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Docs',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis = dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-tfidf-umap3d-docs.html')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tOv_0DniIZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ],
      "metadata": {
        "id": "u-f5EGcS2bNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=3, metric='cosine')\n",
        "dbscan.fit(doc_vectors)\n",
        "num_doc_clusters = np.max(dbscan.labels_)+1\n",
        "print(f\"Hay {num_doc_clusters} clusters\")\n",
        "\n",
        "idxs_per_cluster = {j: np.where(dbscan.labels_==j)[0] for j in range(num_doc_clusters)}\n",
        "docs_per_cluster = {j: [docs[idx] for idx in idxs_per_cluster[j]] for j in idxs_per_cluster.keys()}"
      ],
      "metadata": {
        "id": "Xb0a7yfE2bNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", max_words=1000)\n",
        "\n",
        "w, h = factor_int(num_doc_clusters)\n",
        "fig, axs = plt.subplots(w, h, figsize=(6*w, 3*h))\n",
        "\n",
        "for j,ax in zip(idxs_per_cluster.keys(),axs.flatten()):\n",
        "    wc.generate(' '.join(docs_per_cluster[j]))\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Cluster {j}\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QtvOBJ2y2bNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualizar clusters en la reducción de dimensionalidad\n",
        "\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "umap = UMAP(n_components=3, metric='cosine')\n",
        "X_umap = umap.fit_transform(doc_vectors)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "trace = go.Scatter3d(\n",
        "    x=X_umap[:,0],\n",
        "    y=X_umap[:,1],\n",
        "    z=X_umap[:,2],\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'size': 3,\n",
        "        'opacity': 0.25,\n",
        "        'color': 'gray'\n",
        "    },\n",
        "    hovertemplate='%{text}<extra></extra>',\n",
        "    text = [f\"{docs_raw[j][:75]}\" for j in range(X_umap.shape[0])]\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "for j in idxs_per_cluster.keys():\n",
        "    Xs = X_umap[idxs_per_cluster[j],0]\n",
        "    Ys = X_umap[idxs_per_cluster[j],1]\n",
        "    Zs = X_umap[idxs_per_cluster[j],2]\n",
        "    plot_figure.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=Xs,\n",
        "            y=Ys,\n",
        "            z=Zs,\n",
        "            mode='markers',\n",
        "            marker={\n",
        "                'size': 3,\n",
        "                'opacity': 0.75\n",
        "            },\n",
        "            hovertemplate='%{text}<extra></extra>',\n",
        "            text = [f\"{docs_raw[j][:75]}\" for j in range(X_umap.shape[0])]\n",
        "        )\n",
        "    )\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    title = 'Wikipedia Docs',\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis = dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='wiki-tfidf-umap3d-docs-clusters.html')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Jr74KegwKnDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus 2: 20newsgroups\n",
        "<h2>Features como clasificación</h2>\n",
        "\n",
        "Finalmente usamos los vectores de documentos como features para un clasificador. Hasta el momento, sólo *sabíamos* hacer la clasificación con los $n$-gramas."
      ],
      "metadata": {
        "id": "izAWHrgV2rVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# train_docs = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "# test_docs = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "train_docs = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=['sci.med', 'sci.space'])\n",
        "test_docs = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=['sci.med', 'sci.space'])\n",
        "\n",
        "y_train = train_docs.target\n",
        "y_test = test_docs.target\n",
        "\n",
        "y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "MX9836s92tpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs.data[0]"
      ],
      "metadata": {
        "id": "aljiJDsCOLiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoW"
      ],
      "metadata": {
        "id": "m8rsKbVqP7yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(max_features=1000,stop_words='english')\n",
        "X_train = cv.fit_transform(train_docs.data)\n",
        "X_test = cv.transform(test_docs.data)"
      ],
      "metadata": {
        "id": "xQ37ukmbP7c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, average='weighted')}\")"
      ],
      "metadata": {
        "id": "DEDUACIvQD28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualizar clases en la reducción de dimensionalidad\n",
        "!pip install -qq umap-learn\n",
        "\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "umap = UMAP(n_components=3, metric='cosine')\n",
        "X_umap = umap.fit_transform(X_train)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "plot_figure = go.Figure(layout=layout)\n",
        "\n",
        "for j in np.unique(y_train):\n",
        "    Xs = X_umap[y_train==j,0].reshape(-1,)\n",
        "    Ys = X_umap[y_train==j,1].reshape(-1,)\n",
        "    Zs = X_umap[y_train==j,2].reshape(-1,)\n",
        "    plot_figure.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=Xs,\n",
        "            y=Ys,\n",
        "            z=Zs,\n",
        "            mode='markers',\n",
        "            marker={\n",
        "                'size': 3,\n",
        "                'opacity': 0.75\n",
        "            },\n",
        "            hovertemplate='%{text}<extra></extra>',\n",
        "            text = [f\"{train_docs.data[j][:75]}\" for j in range(Xs.shape[0])],\n",
        "            name = train_docs.target_names[j]\n",
        "        )\n",
        "    )\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis = dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='20ng-bow-umap3d-docs-classes.html')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nc4kLZXCQMRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-idf"
      ],
      "metadata": {
        "id": "fpO6P5CoP5KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "cv = TfidfVectorizer(max_features=1000,stop_words='english')\n",
        "X_train = cv.fit_transform(train_docs.data)\n",
        "X_test = cv.transform(test_docs.data)"
      ],
      "metadata": {
        "id": "TC6M_pkG3kPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, average='weighted')}\")"
      ],
      "metadata": {
        "id": "I9A7jVHZC6VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al usar un método interpretable, como regresión logística, podemos obtener la importancia de las variables, en este caso, las palabras del vocabulario."
      ],
      "metadata": {
        "id": "tPEyOpkGuwNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_.shape"
      ],
      "metadata": {
        "id": "-A6ewiFoucrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras que más influyen en la clasificación de la clase *positiva*"
      ],
      "metadata": {
        "id": "AzCqFs3Du9JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_importance = zip(cv.get_feature_names_out() ,lr.coef_.reshape(-1,))\n",
        "word_importance = sorted(word_importance, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "word_importance[:10]"
      ],
      "metadata": {
        "id": "k6fU5FL9uD3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualizar clases en la reducción de dimensionalidad\n",
        "\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "umap = UMAP(n_components=3, metric='cosine')\n",
        "X_umap = umap.fit_transform(X_train)\n",
        "\n",
        "plotly.offline.init_notebook_mode()\n",
        "\n",
        "\n",
        "layout = go.Layout(\n",
        "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        ")\n",
        "\n",
        "plot_figure = go.Figure(layout=layout)\n",
        "\n",
        "for j in np.unique(y_train):\n",
        "    Xs = X_umap[y_train==j,0].reshape(-1,)\n",
        "    Ys = X_umap[y_train==j,1].reshape(-1,)\n",
        "    Zs = X_umap[y_train==j,2].reshape(-1,)\n",
        "    plot_figure.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=Xs,\n",
        "            y=Ys,\n",
        "            z=Zs,\n",
        "            mode='markers',\n",
        "            marker={\n",
        "                'size': 3,\n",
        "                'opacity': 0.75\n",
        "            },\n",
        "            hovertemplate='%{text}<extra></extra>',\n",
        "            text = [f\"{train_docs.data[j][:75]}\" for j in range(Xs.shape[0])],\n",
        "            name = train_docs.target_names[j]\n",
        "        )\n",
        "    )\n",
        "\n",
        "plot_figure.update_layout(\n",
        "    scene = dict(\n",
        "        xaxis = dict(visible=False),\n",
        "        yaxis = dict(visible=False),\n",
        "        zaxis = dict(visible=False)\n",
        "        )\n",
        "    )\n",
        "\n",
        "plotly.offline.plot(plot_figure, filename='20ng-tfidf-umap3d-docs-classes.html')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C3q11cisNXOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟥 Ejercicios Adicionales"
      ],
      "metadata": {
        "id": "K9gJCAjXRDzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea de clasificación con el corpus `20newsgroups`**. Probar las siguientes estrategias y en cada caso medir el F1 score:\n",
        "\n",
        "1. Todas las clases, sin quitar *headers*, *quotes*, *footers*. Comparar:\n",
        " * BOW\n",
        " * TF-IDF\n",
        " * BOW + PCA\n",
        " * TF-IDF + PCA\n",
        " * BOW + t-SNE\n",
        " * TF-IDF + t-SNE\n",
        "2. Las mismas 6 estrategias del paso anterior, quitando *headers*, *quotes*, *footers*.\n",
        "3. Escoge dos clases que crees que se diferencien muy bien entre sí con estos modelos. ¿Qué clases escogiste y por qué? Compara BOW y TF-IDF para la clasificación binaria.\n",
        "4. Compara tu clasificador de la tarea pasada con el mejor clasificador del paso 3.\n",
        "5. Escoge ahora dos clases que crees que no se diferencien entre sí con estos modelos. ¿Qué clases escogiste y por qué? Compara BOW y TF-IDF para la clasificación binaria. ¿Qué tanto bajó el rendimiento respecto al paso 3?\n",
        "6. En tu mejor clasificador del paso 3, prueba bajando y subiendo el parámetro `max_features` ¿qué efecto tiene esto en la tarea de clasificación?\n",
        "7. ¿Qué efecto tiene lematizar el texto en la tarea de clasificación? Prueba con tu mejor clasificador binario.\n",
        "\n",
        "En cada uno de los modelos BOW/TF-IDF que construyas puedes ajustar el hiperparámetro `max_features`.\n",
        "\n",
        "**Information Retrieval con el corpus `20newsgroups`** Entrena un modelo BOW y un TF-IDF con todos los documentos juntos de `train` y `test`. Realiza algunas consultas al modelo para obtener los documentos más relevantes para tu busqueda. Reporta algunos casos que creas interesantes y explica porque los consideras interesantes.\n",
        "\n",
        "**Análisis de sentimientos con BOW/TFIDF**. Usando el corpus de la tarea anterior (el de turismo), entrena un clasificador de Machine Learning con los embeddings BOW/TF-IDF, ¿mejora el rendimiento respecto al que presentaste en clase?\n"
      ],
      "metadata": {
        "id": "Q6UXR9lA0NvC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQFi-8gSRLGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}